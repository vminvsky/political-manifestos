{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "os.chdir('C:/Users/Venia/Desktop/political-manifestos')\n",
    "text=pd.read_csv(\"data/english_text.csv\")\n",
    "core = pd.read_csv(\"data/english_core.csv\")\n",
    "metadata = pd.read_csv(\"data/english_metadata.csv\")\n",
    "\n",
    "metadata['language'] = metadata['language'].fillna(\"english\")\n",
    "french_keys = metadata[metadata['language']=='french']['key'].values\n",
    "\n",
    "words_to_remove = ['canada','canadians','america','americans','britain']\n",
    "country_list = ['Canada','United Kingdom','United States']\n",
    "\n",
    "df = pd.merge(text,core, on = 'key')\n",
    "df = df[~df['key'].isin(french_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import re\n",
    "\n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "    x = x.replace(\"\\n\", \" \")\n",
    "    x = x.replace('\\t', ' ')\n",
    "    x = x.replace(\"\\'\", '')\n",
    "    x=x.strip()\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "    x = regex.sub('', x)\n",
    "    x = x.translate(remove_digits)\n",
    "    return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_names = pd.read_csv(\"country_names.csv\")\n",
    "country_names = country_names[['countryLabel','capitalLabel','demonym']]\n",
    "countries = country_names[['countryLabel','demonym']].drop_duplicates().reset_index()\n",
    "demonyms = countries.groupby('countryLabel')['demonym'].apply(list).reset_index()\n",
    "demonyms['list'] = demonyms.apply(lambda x: [x['countryLabel']] + x['demonym'], axis =1)\n",
    "demonyms['list'] = demonyms['list'].apply(lambda x: [k.lower() for k in x])\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "df['text_clean_no_stop'] = df['text_clean'].str.replace(pat,'')\n",
    "df['text_clean_no_stop'] = df['text_clean_no_stop'].str.replace(' +', ' ')\n",
    "df['tokenized']=df['text_clean_no_stop'].apply(lambda x: x.split(' '))\n",
    "countries = demonyms['list'].apply(lambda x: x[0]).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in demonyms['list']:\n",
    "    df[row[0]] = 0 \n",
    "for row in demonyms['list']:\n",
    "    country = row[0]\n",
    "    for val in row:\n",
    "        df[country] += df['tokenized'].apply(lambda x: x.count(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].apply(lambda x: str(x)[0:4])\n",
    "country= df[df['countryname'] == 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_windows(country_df, country_name, n=4):\n",
    "    list_of_instances = []\n",
    "    metadata_list = []\n",
    "    for i,j in country[['tokenized','countryname','date','partyname']].iterrows():\n",
    "        metadata = j[['countryname','date','partyname']].values\n",
    "        indices = [k for k, x in enumerate(j['tokenized']) if x == country_name]\n",
    "        for i in indices:\n",
    "            list_of_instances.append([k for k in j['tokenized'][i-n:i+n] if k != country_name])\n",
    "            metadata_list.append(list(metadata))\n",
    "    list_exploded = [k for j in list_of_instances for k in j]\n",
    "    return metadata_list, list_of_instances, list_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, list_of_contexts, list_exploded = context_windows(country,'iraq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(list_exploded):\n",
    "    word_count  = {}\n",
    "    for val in list_exploded:\n",
    "        if val in word_count.keys():\n",
    "            word_count[val] += 1\n",
    "        else:\n",
    "            word_count.update({val: 1})\n",
    "    sorted_dict = dict(sorted(word_count.items(), key = lambda item:item[1], reverse=True))\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_words = top_words(list_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_contexts = pd.DataFrame(metadata,columns = ['country','date','party'])\n",
    "country_contexts['contexts'] = list_of_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_words = country_contexts.groupby(['date', 'party'])['contexts'].apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "election_words = election_words.apply(lambda x: Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_words = election_words.apply(lambda x: dict(sorted(x.items(), key = lambda item:item[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "top_election_words = election_words.apply(lambda x: [k[0] for k in list(islice(x.items(), 4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    party           \n",
       "199211  Republican Party           [president, bush, quagmire, indefinite]\n",
       "199611  Democratic Party                [iran, delivery, nuclear, weapons]\n",
       "        Republican Party                       [iran, syria, libya, north]\n",
       "200011  Democratic Party            [mass, destruction, delivery, systems]\n",
       "        Republican Party                  [friends, community, long, also]\n",
       "200411  Democratic Party    [administration, war, afghanistan, challenges]\n",
       "        Republican Party           [afghanistan, nations, america, forces]\n",
       "200811  Democratic Party                   [war, ending, end, responsibly]\n",
       "        Republican Party         [offer, special, circumstances, conflict]\n",
       "201211  Democratic Party              [war, responsibly, forces, charting]\n",
       "        Republican Party           [afghanistan, country, provide, nature]\n",
       "201611  Democratic Party                 [syria, partners, destroy, isisâ€™]\n",
       "        Republican Party              [afghanistan, support, force, guard]\n",
       "Name: contexts, dtype: object"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_election_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment = top_election_words.explode().apply(sid.polarity_scores).apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = sentiment.reset_index().groupby(['date','party'])['contexts'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    party           \n",
       "196411  Democratic Party    0.191225\n",
       "        Republican Party    0.095850\n",
       "196811  Democratic Party    0.000000\n",
       "        Republican Party   -0.014275\n",
       "197211  Democratic Party    0.000000\n",
       "        Republican Party   -0.149850\n",
       "197611  Democratic Party   -0.259950\n",
       "        Republican Party   -0.149850\n",
       "198011  Democratic Party   -0.149850\n",
       "        Republican Party    0.000000\n",
       "198411  Democratic Party   -0.149850\n",
       "        Republican Party    0.000000\n",
       "198811  Democratic Party    0.111750\n",
       "        Republican Party   -0.100475\n",
       "199211  Democratic Party   -0.120050\n",
       "        Republican Party    0.000000\n",
       "199611  Democratic Party    0.175000\n",
       "        Republican Party    0.114700\n",
       "200011  Democratic Party   -0.223950\n",
       "        Republican Party   -0.085000\n",
       "200411  Republican Party   -0.079550\n",
       "200811  Republican Party    0.119175\n",
       "201211  Democratic Party    0.000000\n",
       "        Republican Party    0.079550\n",
       "201611  Republican Party    0.119175\n",
       "Name: contexts, dtype: float64"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"for_sentiment_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>countryname</th>\n",
       "      <th>edate</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>key2</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61320_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>preamble</td>\n",
       "      <td>61320_200811_0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>61320_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>come</td>\n",
       "      <td>61320_200811_0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>61320_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>together</td>\n",
       "      <td>61320_200811_0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61320_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>defining</td>\n",
       "      <td>61320_200811_0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>61320_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>moment</td>\n",
       "      <td>61320_200811_0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27825</th>\n",
       "      <td>27</td>\n",
       "      <td>61620_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>care</td>\n",
       "      <td>61620_200811_27</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27826</th>\n",
       "      <td>27</td>\n",
       "      <td>61620_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>respect</td>\n",
       "      <td>61620_200811_27</td>\n",
       "      <td>0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27827</th>\n",
       "      <td>27</td>\n",
       "      <td>61620_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>earned</td>\n",
       "      <td>61620_200811_27</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27828</th>\n",
       "      <td>27</td>\n",
       "      <td>61620_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>service</td>\n",
       "      <td>61620_200811_27</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27829</th>\n",
       "      <td>27</td>\n",
       "      <td>61620_200811</td>\n",
       "      <td>United States</td>\n",
       "      <td>04/11/2008</td>\n",
       "      <td>america.</td>\n",
       "      <td>61620_200811_27</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27830 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0           key    countryname       edate tokenized  \\\n",
       "0               0  61320_200811  United States  04/11/2008  preamble   \n",
       "1               0  61320_200811  United States  04/11/2008      come   \n",
       "2               0  61320_200811  United States  04/11/2008  together   \n",
       "3               0  61320_200811  United States  04/11/2008  defining   \n",
       "4               0  61320_200811  United States  04/11/2008    moment   \n",
       "...           ...           ...            ...         ...       ...   \n",
       "27825          27  61620_200811  United States  04/11/2008      care   \n",
       "27826          27  61620_200811  United States  04/11/2008   respect   \n",
       "27827          27  61620_200811  United States  04/11/2008    earned   \n",
       "27828          27  61620_200811  United States  04/11/2008   service   \n",
       "27829          27  61620_200811  United States  04/11/2008  america.   \n",
       "\n",
       "                  key2  sentiment  \n",
       "0       61320_200811_0     0.0000  \n",
       "1       61320_200811_0     0.0000  \n",
       "2       61320_200811_0     0.0000  \n",
       "3       61320_200811_0     0.0000  \n",
       "4       61320_200811_0     0.0000  \n",
       "...                ...        ...  \n",
       "27825  61620_200811_27     0.4939  \n",
       "27826  61620_200811_27     0.4767  \n",
       "27827  61620_200811_27     0.0000  \n",
       "27828  61620_200811_27     0.0000  \n",
       "27829  61620_200811_27     0.0000  \n",
       "\n",
       "[27830 rows x 7 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df['tokenized'].apply(sid.polarity_scores).apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key2\n",
       "61320_200811_0     0.025764\n",
       "61320_200811_1     0.031206\n",
       "61320_200811_10    0.016954\n",
       "61320_200811_11    0.037287\n",
       "61320_200811_12    0.027365\n",
       "61320_200811_13    0.016122\n",
       "61320_200811_2     0.038371\n",
       "61320_200811_3     0.028138\n",
       "61320_200811_4     0.040035\n",
       "61320_200811_5     0.047219\n",
       "61320_200811_6     0.035080\n",
       "61320_200811_7     0.003643\n",
       "61320_200811_8     0.005892\n",
       "61320_200811_9     0.042842\n",
       "61620_200811_14    0.019091\n",
       "61620_200811_15    0.036707\n",
       "61620_200811_16    0.033357\n",
       "61620_200811_17    0.034850\n",
       "61620_200811_18    0.014607\n",
       "61620_200811_19    0.022966\n",
       "61620_200811_20    0.027242\n",
       "61620_200811_21    0.025334\n",
       "61620_200811_22    0.038055\n",
       "61620_200811_23    0.043667\n",
       "61620_200811_24    0.039806\n",
       "61620_200811_25   -0.005547\n",
       "61620_200811_26    0.010474\n",
       "61620_200811_27    0.022557\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['key2'])['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
